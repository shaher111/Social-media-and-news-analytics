{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Qatar\"\n",
    "users = \"\"\n",
    "topic = \"HIA\"\n",
    "subtopic = \"\"\n",
    "query_language = \"All\"\n",
    "target_languages = \"English,Arabic\"\n",
    "num_tweets = 50\n",
    "days = 7\n",
    "result_type = \"recent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_CODES={\"All\":\"\",\"Afrikaans\":\"af\",\"Arabic\":\"ar\",\"Assamese\":\"as\",\"Bangla\":\"bn\",\"Bosnian(Latin)\":\"bs\",\"Bulgarian\":\"bg\",\"Cantonese(Traditional)\":\"yue\",\"Catalan\":\"ca\",\"Chinese Simplified\":\"zh-Hans\",\"Chinese Traditional\":\"zh-Hant\",\"Croatian\":\"hr\",\"Czech\":\"cs\",\"Dari\":\"prs\",\"Danish\":\"da\",\"Dutch\":\"nl\",\"English\":\"en\",\"Estonian\":\"et\",\"Fijian\":\"fj\",\"Filipino\":\"fil\",\"Finnish\":\"fi\",\"French\":\"fr\",\"German\":\"de\",\"Greek\":\"el\",\"Gujarati\":\"gu\",\"Haitian Creole\":\"ht\",\"Hebrew\":\"he\",\"Hindi\":\"hi\",\"Hmong Daw\":\"mww\",\"Hungarian\":\"hu\",\"Icelandic\":\"is\",\"Indonesian\":\"id\",\"Irish\":\"ga\",\"Italian\":\"it\",\"Japanese\":\"ja\",\"Kannada\":\"kn\",\"Kazakh\":\"kk\",\"Klingon\":\"tlh-Latn\",\"Klingon(plqaD)\":\"tlh-Piqd\",\"Korean\":\"ko\",\"Kurdish(Central)\":\"ku\",\"Kurdish(Northern)\":\"kmr\",\"Latvian\":\"lv\",\"Lithuanian\":\"lt\",\"Malagasy\":\"mg\",\"Malay\":\"ms\",\"Malayalam\":\"ml\",\"Maltese\":\"mt\",\"Maori\":\"mi\",\"Marathi\":\"mr\",\"Norwegian\":\"nb\",\"Odia\":\"or\",\"Pashto\":\"ps\",\"Persian\":\"fa\",\"Polish\":\"pl\",\"Portuguese(Brazil)\":\"pt-br\",\"Portuguese(Portugal)\":\"pt-pt\",\"Punjabi\":\"pa\",\"Queretaro Otomi\":\"otq\",\"Romanian\":\"ro\",\"Russian\":\"ru\",\"Samoan\":\"sm\",\"Serbian(Cyrillic)\":\"sr-Cyrl\",\"Serbian(Latin)\":\"sr-Latn\",\"Slovak\":\"sk\",\"Slovenian\":\"sl\",\"Spanish\":\"es\",\"Swahili\":\"sw\",\"Swedish\":\"sv\",\"Tahitian\":\"ty\",\"Tamil\":\"ta\",\"Telugu\":\"te\",\"Thai\":\"th\",\"Tongan\":\"to\",\"Turkish\":\"tr\",\"Ukrainian\":\"uk\",\"Urdu\":\"ur\",\"Vietnamese\":\"vi\",\"Welsh\":\"cy\",\"Yucatec Maya\":\"yua\"}\n",
    "topic = topic.lower()\n",
    "# username = user\n",
    "userslist = users.split(',')\n",
    "query_language = LANGUAGE_CODES.get(query_language, \"\")\n",
    "target_languages = [LANGUAGE_CODES.get(lang, \"\") for lang in target_languages.split(\",\")]\n",
    "if \"en\" not in target_languages:\n",
    "  target_languages.append(\"en\") # we always include english in target languages\n",
    "num_tweets = int(num_tweets)\n",
    "max_days = int(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (query == \"\") or (users == \"\") # Can either search by query or by user, not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosmos Temp\n",
    "COSMOS_URL = \"https://cosmos-socialmedia-dev-01.documents.azure.com:443/\"\n",
    "COSMOS_KEY = \"k4nj8hc0R0hVBd888i1CoB3Pkv3aimDG1eE8BRNat54POhxiQaggqP251yHaPE9bzQwVXqtSUELZKfIaRfvAQg==\"\n",
    "COSMOS_DATABASE_NAME = \"databasesocialmedia01\"\n",
    "COSMOS_CONTAINER_NAME = \"KSA_Tweets\"\n",
    "\n",
    "\n",
    "# Text Analytics Temp\n",
    "TEXT_ANALYTICS_KEY = \"dd902b6fa8bb429aac0bba75532d456e\"\n",
    "TEXT_ANALYTICS_ENDPOINT = \"https://textanalytics-socialmedia-dev-01.cognitiveservices.azure.com/\"\n",
    "TEXT_ANALYTICS_REGION = \"westeurope\"\n",
    "\n",
    "#Translator\n",
    "TRANSLATOR_KEY = \"7d5eef61105d44a78840f7a8e49a8d9b\"\n",
    "TRANSLATOR_ENDPOINT = \"https://api.cognitive.microsofttranslator.com/\"\n",
    "TRANSLATOR_REGION = \"westeurope\" \n",
    "\n",
    "# Twitter Temp\n",
    "TWITTER_API_KEY = \"3pJCEcL9129CXWzFNg0mRSIFC\"\n",
    "TWITTER_API_SECRET_KEY = \"c8z6vrQLqu8pfnfaWIK4qksgfqS7hrvtDlstbIP1l4iAqx7WFQ\"\n",
    "TWITTER_ACCESS_TOKEN = \"1621038712728571905-suTbQDs30WjsiArwqQDcMXf1lITkHM\"\n",
    "TWITTER_ACCESS_TOKEN_SECRET = \"nUibZQYC7prXLImzoPDFENaoBB1xxA0i1UqBMv3TawytW\"\n",
    "TWITTER_ACCESS_BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAJjToAEAAAAA1XtTjINXfjBjqammKhVfECdJqus%3D1llMZPdGaggEZect52d5KtjdjyyRBXdN39OHNVMCjox2yL98Id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Maps Temp\n",
    "MAPS_KEY = \"-UbQBKw-d24t4ZfppLDxHgTYVbXm63ZKg24V2xt9NHY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import copy\n",
    "import tweepy\n",
    "import urllib.parse\n",
    "import sys, time, json, requests, uuid\n",
    "\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime, date, timedelta # don't import time here. It messes with the default library\n",
    "\n",
    "\n",
    "from azure.cosmos import CosmosClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_client():\n",
    "    \"\"\"\n",
    "    Returns: text analytics client\n",
    "    \"\"\"\n",
    "    ta_credential = AzureKeyCredential(TEXT_ANALYTICS_KEY)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=TEXT_ANALYTICS_ENDPOINT, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "text_analytics_client = authenticate_client()\n",
    "# TODO: Add opinion mining\n",
    "def get_sentiment(inp_text):\n",
    "    #Parameters: \n",
    "    #  inp_text: text to analyze\n",
    "    #Returns:\n",
    "    #  sentiment, sentiment score\n",
    "    documents = [inp_text]\n",
    "    response = text_analytics_client.analyze_sentiment(documents = documents)[0]  \n",
    "    try:\n",
    "        overallscore = response.confidence_scores.positive + (0.5*response.confidence_scores.neutral) # check logic of this\n",
    "        return response.sentiment, overallscore\n",
    "    except Exception as err:\n",
    "        print(\"Encountered Sentiment exception. {}\".format(err))\n",
    "        return \"Neutral\",0.5\n",
    "def get_ner(inp_text):\n",
    "    #Parameters: \n",
    "    #  inp_text: text to analyze\n",
    "    #Returns:\n",
    "    #  NER Results as a list of dictionaries with keys: text, category, subcategory, length, offset, confidence\n",
    "    try:\n",
    "        documents = [inp_text]\n",
    "        result = text_analytics_client.recognize_entities(documents = documents)[0]  \n",
    "        return [{\"text\": x.text, \"category\": x.category, \"subcategory\": x.subcategory, \"length\": x.length, \"offset\": x.offset, \"confidence_score\": x.confidence_score} for x in result.entities]\n",
    "    except Exception as err:\n",
    "        print(\"Encountered NER exception. {}\".format(err))\n",
    "    return []\n",
    "\n",
    "def get_routes_from_text(inp_text):\n",
    "    try:\n",
    "        documents = [inp_text]\n",
    "        result = text_analytics_client.recognize_entities(documents=documents)[0]\n",
    "\n",
    "        # Filter entities that are of type 'Location'\n",
    "        location_entities = [x for x in result.entities if x.category == 'Location' and x.subcategory == 'GPE']\n",
    "\n",
    "        # Initialize route variables\n",
    "        from_entity = None\n",
    "        to_entity = None\n",
    "        routes = []\n",
    "\n",
    "        # Find consecutive 'from' and 'to' entities to create route tuples\n",
    "        for entity in location_entities:\n",
    "            entity_position = entity.offset\n",
    "            entity_length = entity.length\n",
    "            entity_text = inp_text[entity_position: entity_position + entity_length]\n",
    "\n",
    "            if from_entity is None:\n",
    "                from_entity = entity_text\n",
    "            elif to_entity is None and entity_position > from_entity_offset:\n",
    "                to_entity = entity_text\n",
    "                route = (from_entity, to_entity)\n",
    "                routes.append(route)\n",
    "                from_entity = entity_text  # Update from_entity for multi-word locations\n",
    "                to_entity = None\n",
    "\n",
    "            from_entity_offset = entity_position\n",
    "\n",
    "        return routes\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered NER exception. {}\".format(err))\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_key_phrases(inp_text):\n",
    "    #Parameters: \n",
    "    #  inp_text: text to analyze\n",
    "    #Returns:\n",
    "    #  List of key phrases\n",
    "    try:\n",
    "      documents = [inp_text]\n",
    "      response = text_analytics_client.extract_key_phrases(documents = documents)[0] \n",
    "      if not response.is_error:\n",
    "          return response.key_phrases\n",
    "      else:\n",
    "          print(response.id, response.error)\n",
    "    except Exception as err:\n",
    "      print(\"Encountered Translation exception. {}\".format(err))\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cosmos(objects, container): # insert tweets/users to cosmos\n",
    "    for obj in objects:\n",
    "        if obj:\n",
    "            response = container.upsert_item(body=obj) # use upsert so that insert or update\n",
    "    print(\"Inserted data to cosmos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet Entities processing\n",
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "def get_maps_response(inp):\n",
    "    url = \"https://atlas.microsoft.com/search/fuzzy/json?&subscription-key=\"+MAPS_KEY+\"&api-version=1.0&language=en-US&query=\"+inp\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation(inp_text, to_languages):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    inp_text: text to be translated\n",
    "    to_languages: list of languages to translate to\n",
    "    Returns: {lang_code: translation}, language code of the original text\n",
    "    Call to translator cognitive service detects language and translates to the target languages. \n",
    "    Result is a dictionary of language codes to translated text, along with the language detected.\n",
    "    \"\"\"\n",
    "    # Translator setup\n",
    "    translator_path = \"/translate\"\n",
    "    translator_url = TRANSLATOR_ENDPOINT + translator_path\n",
    "    params = {\n",
    "    \"api-version\": \"3.0\",\n",
    "    \"to\": to_languages\n",
    "    }\n",
    "    headers = {\n",
    "    'Ocp-Apim-Subscription-Key': TRANSLATOR_KEY,\n",
    "    'Ocp-Apim-Subscription-Region': TRANSLATOR_REGION,\n",
    "    'Content-type': 'application/json',\n",
    "    'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "    # create and send request\n",
    "    body = [{\n",
    "    'text': inp_text\n",
    "    }]\n",
    "    request = requests.post(translator_url, params=params, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    # only ever one string sent for translation, so only list of length 1\n",
    "    try:\n",
    "        from_language = response[0][\"detectedLanguage\"][\"language\"]\n",
    "        translations = response[0][\"translations\"]\n",
    "        res = {} # dict with language as key and translated text as value e.g. {\"ar\": \"arabic text\"}\n",
    "        for trans in translations:\n",
    "            res[trans['to']] = trans['text']\n",
    "        #res[from_language] = inp_text # also include the original text and language in translations - overkill?\n",
    "        return res, from_language # return the translated text, as well as the language it was translated from\n",
    "    except Exception as err:\n",
    "        print(\"Encountered an exception. {}\".format(err))\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CosmosClient(COSMOS_URL, {'masterKey': COSMOS_KEY})\n",
    "database = client.get_database_client(COSMOS_DATABASE_NAME)\n",
    "\n",
    "tweet_container_client = database.get_container_client(container=COSMOS_CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = TWITTER_API_KEY\n",
    "consumer_secret = TWITTER_API_SECRET_KEY\n",
    "access_token = TWITTER_ACCESS_TOKEN\n",
    "access_token_secret = TWITTER_ACCESS_TOKEN_SECRET\n",
    "bearer_token = TWITTER_ACCESS_BEARER_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweepy_client =tweepy.Client(bearer_token=bearer_token, consumer_key=consumer_key, consumer_secret=consumer_secret, access_token=access_token, access_token_secret=access_token_secret, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the since_datetime to be 7 days in the past from the current date and time\n",
    "since_datetime = datetime.utcnow() - timedelta(days=7)\n",
    "\n",
    "# Get the top 1 tweet from Cosmos DB that meets the criteria\n",
    "cosmos_query = f\"SELECT TOP 1 c.originalid, c.created_at FROM c WHERE c.document_type = 'tweet' and c.query= '{query}' and c.created_at > '{since_datetime.isoformat()}' ORDER BY c.created_at DESC\"\n",
    "items = list(tweet_container_client.query_items(cosmos_query, enable_cross_partition_query=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the tweet ID if available\n",
    "if items:\n",
    "    since_id = str(items[0]['originalid'])\n",
    "else:\n",
    "    # Handle the case when no valid tweet ID is available\n",
    "    since_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\"Africa\",\"Arabian Gulf\",\"Asia\",\"Central America\",\"Europe\",\"Middle East\",\"North America\",\"Oceania\",\"South America\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoji = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        \"]+\", re.UNICODE)\n",
    "    return re.sub(emoji, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_entry(topic, status, query_search, container, query_language=None, target_languages=[], username=\"\"):\n",
    "    tweet = status\n",
    "    id_str = str(int(tweet[\"id\"]) + abs(hash(topic)))\n",
    "    # Check for tweets which have already been added\n",
    "    search_query = 'select * from items where items.id=\"{0}\"'.format(id_str)\n",
    "    items = list(container.query_items(search_query, enable_cross_partition_query=True))\n",
    "    dt2 = datetime.now()\n",
    "    ts = int(time.mktime(dt2.timetuple()))\n",
    "    at = dt2.strftime(\"%m/%d/%Y, %H:%M:%S %Z\")\n",
    "    tweet['inserted_to_CosmosDB_at'] = at\n",
    "    tweet['inserted_to_CosmosDB_ts'] = ts\n",
    "    new_tweet = True\n",
    "    if len(items) > 0:\n",
    "        # For existing tweets, assuming tweet text the same, so don't re-process\n",
    "        # Update only the retweet and favorite counts\n",
    "        print(\"Old Tweet\")\n",
    "        updatedtweet = items[0]\n",
    "        updatedtweet[\"retweet_count\"] = tweet[\"public_metrics\"][\"retweet_count\"]\n",
    "        updatedtweet[\"favorite_count\"] = tweet[\"public_metrics\"][\"like_count\"]\n",
    "        tweet = updatedtweet\n",
    "        user_obj = None  # don't need to re-insert the user if already seen before\n",
    "        new_tweet = False\n",
    "    elif not tweet[\"text\"].lower().startswith(\"rt \"):\n",
    "        print(\"New Tweet\")\n",
    "        new_tweet = True\n",
    "        tweet[\"originalid\"] = tweet[\"id\"]\n",
    "        tweet[\"id\"] = str(int(tweet[\"id\"]) + abs(hash(topic)))  # artificially creating our own ID\n",
    "        tweet[\"topickey\"] = topic\n",
    "        tweet[\"subtopic\"] = subtopic\n",
    "        tweet[\"month_year\"] = str(str(parse(tweet[\"created_at\"]).month) + \"_\" + str(parse(tweet[\"created_at\"]).year))\n",
    "        tweet[\"replies\"] = []  # check if we want to keep this like this\n",
    "        tmp_text = tweet[\"text\"].replace('\\n', '. ').replace('\\r', '.').replace('..', '. ').replace(',.', '. ').replace(';.',\n",
    "                                                                                                                   '. ').replace(\n",
    "            '?.', '. ').replace('!.', '. ').replace(':.', '. ').lstrip('.').lstrip(' ')\n",
    "        tmp_text = remove_emojis(tmp_text)\n",
    "        tweet[\"processeed_text\"] = tmp_text\n",
    "        tweet[\"document_type\"] = \"tweet\"\n",
    "        if query_search:\n",
    "            tweet[\"search_type\"] = 'Topic Search'\n",
    "            tweet[\"query\"] = query\n",
    "        else:\n",
    "            tweet[\"search_type\"] = 'User Search'\n",
    "            tweet[\"searched_username\"] = username\n",
    "        tweet_text = tweet[\"processeed_text\"]\n",
    "        # get translation\n",
    "        if not query_language:\n",
    "            # will depend on language detection from the translator call\n",
    "            translations, query_language = get_translation(tweet_text, target_languages)\n",
    "        else:\n",
    "            translations, _ = get_translation(tweet_text, target_languages)\n",
    "        tweet[\"translations\"] = translations\n",
    "        # get named entities. Arabic only supports Person, Location and Organization entities, and seems to be poor, so doing NER on English text\n",
    "        named_entity_obj = {}\n",
    "\n",
    "#         if query_language != \"en\":\n",
    "#             named_entities = get_ner(translations[\"en\"])\n",
    "#             org_language_entities = []\n",
    "#             for ent in named_entities:\n",
    "#                 org_language_ent = copy.deepcopy(ent)\n",
    "#                 print(org_language_ent)\n",
    "#                 org_language_ent[\"text\"] = get_translation(ent[\"text\"], query_language)[0][\n",
    "#                     query_language]  # replace with original language text\n",
    "#                 print(org_language_ent[\"text\"])\n",
    "#                 org_language_entities.append(org_language_ent)\n",
    "#             named_entity_obj[query_language] = org_language_entities\n",
    "        \n",
    "        if query_language != \"en\":\n",
    "            named_entities = get_ner(translations[\"en\"])\n",
    "            org_language_entities = []\n",
    "            for ent in named_entities:\n",
    "                org_language_ent = copy.deepcopy(ent)\n",
    "                #print(org_language_ent)\n",
    "                try:\n",
    "                    org_language_ent[\"text\"] = get_translation(ent[\"text\"], query_language)[0][\n",
    "                        query_language]  # replace with original language text\n",
    "                    #print(org_language_ent[\"text\"])\n",
    "                except KeyError as e:\n",
    "                    ##print(\"KeyError occurred while translating:\", e)\n",
    "                    org_language_ent[\"text\"] = ent[\"text\"]  # Assign original text if translation fails\n",
    "                org_language_entities.append(org_language_ent)\n",
    "            named_entity_obj[query_language] = org_language_entities\n",
    "        else:\n",
    "            named_entities = get_ner(tweet_text)  # list of objects where each object corresponds to an entity\n",
    "\n",
    "            # add location information from azure maps\n",
    "            named_entities_with_location = []\n",
    "            for ent in named_entities:\n",
    "                new_ent = copy.deepcopy(ent)\n",
    "                if ent[\"category\"] == \"Location\" and ent[\"subcategory\"] == \"GPE\":\n",
    "                    ent_text = ent[\"text\"]\n",
    "                    if ent_text not in regions:\n",
    "                        # pass to Azure Maps to get country\n",
    "                        r_json = get_maps_response(ent_text)\n",
    "                        if r_json:  # i.e. got a response\n",
    "                            if r_json[\"summary\"][\"numResults\"] > 0:\n",
    "                                if \"address\" in r_json['results'][0].keys():\n",
    "                                    top_match = r_json['results'][0][\"address\"]\n",
    "                                    if \"country\" in top_match.keys() and \"countryCode\" in top_match.keys():\n",
    "                                        # there is a location detected, so get the country\n",
    "                                        country = top_match[\"country\"]\n",
    "                                        country_code = top_match[\"countryCode\"]\n",
    "                                        new_ent[\"country_azuremaps\"] = country\n",
    "                                        new_ent[\"country_code_azuremaps\"] = country_code\n",
    "                named_entities_with_location.append(new_ent)\n",
    "            named_entity_obj[\"en\"] = named_entities_with_location\n",
    "\n",
    "            \n",
    "\n",
    "            for language in target_languages:\n",
    "                if language != \"en\":\n",
    "                    tmp_entities = []\n",
    "                    for ent in named_entities:\n",
    "                        tmp_ = copy.deepcopy(ent)\n",
    "                        tmp_[\"text\"] = get_translation(ent[\"text\"], language)[0][language]\n",
    "                        tmp_entities.append(tmp_)\n",
    "                    named_entity_obj[language] = tmp_entities\n",
    "\n",
    "        tweet[\"named_entities\"] = named_entity_obj\n",
    "        # Extract routes using get_routes_from_text() function\n",
    "        routes = get_routes_from_text(tweet_text)\n",
    "        tweet[\"routes\"] = routes\n",
    "\n",
    "        # get sentiment. Not supported for Arabic, so do on English. No need to translate back\n",
    "        if query_language != \"en\":\n",
    "            sentiment, sentiment_score = get_sentiment(translations[\"en\"])\n",
    "        else:\n",
    "            sentiment, sentiment_score = get_sentiment(tweet_text)\n",
    "        tweet[\"sentiment\"] = {\"sentiment\": sentiment, \"score\": sentiment_score}\n",
    "        user_obj = tweet[\"User\"]\n",
    "        # print(user_obj)\n",
    "        user_obj[\"topickey\"] = topic\n",
    "        user_obj[\"id\"] = user_obj[\"id\"]\n",
    "        user_obj[\"document_type\"] = \"user\"\n",
    "        user_obj['inserted_to_CosmosDB_at'] = at\n",
    "        user_obj['inserted_to_CosmosDB_ts'] = ts\n",
    "        user_obj[\"month_year\"] = str(str(parse(user_obj[\"created_at\"]).month) + \"_\" + str(parse(user_obj[\"created_at\"]).year))\n",
    "\n",
    "        try:\n",
    "            user_location = tweet[\"User\"][\"location\"]\n",
    "        except KeyError:\n",
    "            user_location = \"\"\n",
    "\n",
    "        # user_location = tweet[\"User\"][\"location\"]\n",
    "        if user_location != \"\" and user_location not in regions:\n",
    "            r_json = get_maps_response(user_location)\n",
    "            if r_json:  # i.e. got a response\n",
    "                if r_json[\"summary\"][\"numResults\"] > 0:\n",
    "                    # there is a location detected, so get the country\n",
    "                    if \"address\" in r_json['results'][0].keys():\n",
    "                        top_match = r_json['results'][0][\"address\"]\n",
    "                        if \"country\" in top_match.keys() and \"countryCode\" in top_match.keys():\n",
    "                            country = top_match[\"country\"]\n",
    "                            country_code = top_match[\"countryCode\"]\n",
    "                            user_obj[\"country_azuremaps\"] = country\n",
    "                            user_obj[\"country_code_azuremaps\"] = country_code\n",
    "        tweet[\"userid\"] = user_obj[\"id\"]\n",
    "    else:\n",
    "        return None, None, False\n",
    "    return tweet, user_obj, new_tweet\n",
    "\n",
    "def process_tweets(topic=\"\", query=\"\", language=\"en\", maxdays=365, maxtweets_persearch=1, user=\"\", query_search=True, container=None, target_languages=[], username=\"\"):\n",
    "    print(\"Working on topic: \" + topic)\n",
    "    end_date = datetime.utcnow() - timedelta(days=maxdays)\n",
    "    all_tweets = []\n",
    "    all_users = []\n",
    "    count = 0\n",
    "    # Reference: https://docs.tweepy.org/en/stable/api.html#API.search\n",
    "    if query_search:\n",
    "    # searching based on the query string\n",
    "        if language:\n",
    "            results = tweepy_client.search_recent_tweets(\n",
    "                    query=query,\n",
    "                    tweet_fields=[\n",
    "                        'attachments',\n",
    "                        'author_id',\n",
    "                        'context_annotations',\n",
    "                        'conversation_id',\n",
    "                        'created_at',\n",
    "                        'entities',\n",
    "                        'geo',\n",
    "                        \n",
    "                        'id',\n",
    "                        'in_reply_to_user_id',\n",
    "                        'lang',\n",
    "                        'note_tweet',\n",
    "                        'possibly_sensitive',\n",
    "                        'public_metrics',\n",
    "                        'referenced_tweets',\n",
    "                        'reply_settings',\n",
    "                        'source',\n",
    "                        'text'\n",
    "                    ],\n",
    "                    expansions=[\n",
    "                        'author_id',\n",
    "                        'referenced_tweets.id',\n",
    "                        'referenced_tweets.id.author_id',\n",
    "                        'entities.mentions.username',\n",
    "                        'entities.note.mentions.username',\n",
    "                        'attachments.media_keys',\n",
    "                        'in_reply_to_user_id',\n",
    "                        'geo.place_id',            ],\n",
    "                    media_fields = [\"url\",\"height\",\"width\",\"public_metrics\",\"media_key\",\"preview_image_url\"], \n",
    "                    user_fields = [\"id\", \"name\", \"public_metrics\", \"username\", \"verified\",\"url\",\"profile_image_url\",\"created_at\",\"description\",\"entities\",\"location\",\"pinned_tweet_id\",\"verified_type\"] ,\n",
    "                    place_fields = [\"contained_within\",\"country\",\"country_code\",\"geo\",\"name\",\"place_type\",\"full_name\"], \n",
    "\n",
    "                max_results = num_tweets,\n",
    "                since_id = since_id\n",
    "                )\n",
    "\n",
    "            # Get users list from the includes object\n",
    "            try:\n",
    "                users = {u[\"id\"]:u.data for u in results.includes['users']}\n",
    "            except KeyError:\n",
    "                users = {}\n",
    "            try:\n",
    "                allMedia = {m[\"media_key\"]: m.data for m in results.includes['media']}\n",
    "            except KeyError:\n",
    "                allMedia = {}    \n",
    "            try:\n",
    "                places = {p[\"id\"]: p.data for p in results.includes['places']}\n",
    "            except KeyError:\n",
    "                places = {}\n",
    "\n",
    "\n",
    "            if results.data is not None:\n",
    "                for status in results.data:\n",
    "                    count += 1\n",
    "                    author_id = status['author_id']\n",
    "\n",
    "                    media_key = None\n",
    "                    if 'entities' in status and 'urls' in status['entities'] and len(status['entities']['urls']) > 1:\n",
    "                        if 'media_key' in status['entities']['urls'][1]:\n",
    "                            media_key = status['entities']['urls'][1]['media_key']\n",
    "\n",
    "                    place_id = None\n",
    "                    if 'geo' in status and 'place_id' in status['geo']:\n",
    "                        place_id = status['geo']['place_id']\n",
    "\n",
    "                    user = users.get(author_id)\n",
    "                    media = allMedia.get(media_key)\n",
    "                    place = places.get(place_id)\n",
    "\n",
    "                    status = json.dumps(status.data)\n",
    "                    status = json.loads(status)\n",
    "                    status['User'] = user\n",
    "                    status['Media'] = media\n",
    "                    status['Place'] = place\n",
    "                    status['tweet_url'] = \"https://twitter.com/\" + str(status['User']['username']) + \"/status/\" + str(status['id'])\n",
    "                    status['query'] = query\n",
    "                    \n",
    "                    tweet_obj, user_obj, new_tweet = build_entry(topic, status, query_search, container, language, target_languages)\n",
    "                    print(new_tweet)\n",
    "                    all_tweets.append(tweet_obj)\n",
    "                    if new_tweet:\n",
    "                        all_users.append(user_obj)\n",
    "                    if datetime.strptime(status['created_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\") < end_date:\n",
    "                        break\n",
    "            else:\n",
    "                print(\"No results found.\")\n",
    "            print(\"Found \"+str(count) +\" tweets for query: \"+ query)\n",
    "            return all_tweets, all_users\n",
    "        else: # search for tweets regardless of tweet language\n",
    "            print(\"Start search\")\n",
    "            results = tweepy_client.search_recent_tweets(\n",
    "                    query=query,\n",
    "                    tweet_fields=[\n",
    "                        'attachments',\n",
    "                        'author_id',\n",
    "                        'context_annotations',\n",
    "                        'conversation_id',\n",
    "                        'created_at',\n",
    "                        'entities',\n",
    "                        'geo',\n",
    "                        \n",
    "                        'id',\n",
    "                        'in_reply_to_user_id',\n",
    "                        'lang',\n",
    "                        'note_tweet',\n",
    "                        'possibly_sensitive',\n",
    "                        'public_metrics',\n",
    "                        'referenced_tweets',\n",
    "                        'reply_settings',\n",
    "                        'source',\n",
    "                        'text'\n",
    "                    ],\n",
    "                    expansions=[\n",
    "                        'author_id',\n",
    "                        'referenced_tweets.id',\n",
    "                        'referenced_tweets.id.author_id',\n",
    "                        'entities.mentions.username',\n",
    "                        'entities.note.mentions.username',\n",
    "                        'attachments.media_keys',\n",
    "                        'in_reply_to_user_id',\n",
    "                        'geo.place_id',            ],\n",
    "                    media_fields = [\"url\",\"height\",\"width\",\"public_metrics\",\"media_key\",\"preview_image_url\"], \n",
    "                    user_fields = [\"id\", \"name\", \"public_metrics\", \"username\", \"verified\",\"url\",\"profile_image_url\",\"created_at\",\"description\",\"entities\",\"location\",\"pinned_tweet_id\",\"verified_type\"],\n",
    "                    place_fields = [\"contained_within\",\"country\",\"country_code\",\"geo\",\"name\",\"place_type\",\"full_name\"], \n",
    "\n",
    "                max_results = num_tweets,\n",
    "                since_id = since_id\n",
    "                )\n",
    "\n",
    "            print(\"After search/results var\")\n",
    "\n",
    "            # Get users list from the includes object\n",
    "            try:\n",
    "                users = {u[\"id\"]:u.data for u in results.includes['users']}\n",
    "            except KeyError:\n",
    "                users = {}\n",
    "            try:\n",
    "                allMedia = {m[\"media_key\"]: m.data for m in results.includes['media']}\n",
    "            except KeyError:\n",
    "                allMedia = {}    \n",
    "            try:\n",
    "                places = {p[\"id\"]: p.data for p in results.includes['places']}\n",
    "            except KeyError:\n",
    "                places = {}\n",
    "\n",
    "\n",
    "            if results.data is not None:\n",
    "                for status in results.data:\n",
    "                    count += 1\n",
    "                    author_id = status['author_id']\n",
    "\n",
    "                    media_key = None\n",
    "                    if 'entities' in status and 'urls' in status['entities'] and len(status['entities']['urls']) > 1:\n",
    "                        if 'media_key' in status['entities']['urls'][1]:\n",
    "                            media_key = status['entities']['urls'][1]['media_key']\n",
    "\n",
    "                    place_id = None\n",
    "                    if 'geo' in status and 'place_id' in status['geo']:\n",
    "                        place_id = status['geo']['place_id']\n",
    "\n",
    "                    user = users.get(author_id)\n",
    "                    media = allMedia.get(media_key)\n",
    "                    place = places.get(place_id)\n",
    "\n",
    "                    status = json.dumps(status.data)\n",
    "                    status = json.loads(status)\n",
    "                    status['User'] = user\n",
    "                    status['Media'] = media\n",
    "                    status['Place'] = place\n",
    "                    status['tweet_url'] = \"https://twitter.com/\" + str(status['User']['username']) + \"/status/\" + str(status['id'])\n",
    "                    status['query'] = query\n",
    "                    tweet_obj, user_obj, new_tweet = build_entry(topic, status, query_search, container, query_language=None, target_languages=target_languages)\n",
    "                    all_tweets.append(tweet_obj)\n",
    "                    all_users.append(user_obj)\n",
    "                    if datetime.strptime(status['created_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\") < end_date:\n",
    "                        break \n",
    "            else:\n",
    "                print(\"No results found.\")\n",
    "            print(\"Found \"+str(count) +\" tweets for query: \"+ query)\n",
    "            return all_tweets, all_users\n",
    "    else:\n",
    "    # getting tweets by user\n",
    "        print(\"getting tweets by user\")\n",
    "        results = tweepy_client.get_users_tweets(\n",
    "                    id=user,\n",
    "                    tweet_fields=[\n",
    "                        'attachments',\n",
    "                        'author_id',\n",
    "                        'context_annotations',\n",
    "                        'conversation_id',\n",
    "                        'created_at',\n",
    "                        'entities',\n",
    "                        'geo',\n",
    "                        \n",
    "                        'id',\n",
    "                        'in_reply_to_user_id',\n",
    "                        'lang',\n",
    "                        'note_tweet',\n",
    "                        'possibly_sensitive',\n",
    "                        'public_metrics',\n",
    "                        'referenced_tweets',\n",
    "                        'reply_settings',\n",
    "                        'source',\n",
    "                        'text'\n",
    "                    ],\n",
    "                    expansions=[\n",
    "                        'author_id',\n",
    "                        'referenced_tweets.id',\n",
    "                        'referenced_tweets.id.author_id',\n",
    "                        'entities.mentions.username',\n",
    "                        'entities.note.mentions.username',\n",
    "                        'attachments.media_keys',\n",
    "                        'in_reply_to_user_id',\n",
    "                        'geo.place_id',            ],\n",
    "                    media_fields = [\"url\",\"height\",\"width\",\"public_metrics\",\"media_key\",\"preview_image_url\"], \n",
    "                    user_fields = [\"id\", \"name\", \"public_metrics\", \"username\", \"verified\",\"url\",\"profile_image_url\",\"created_at\",\"description\",\"entities\",\"location\",\"pinned_tweet_id\",\"verified_type\"],\n",
    "                    place_fields = [\"contained_within\",\"country\",\"country_code\",\"geo\",\"name\",\"place_type\",\"full_name\"], \n",
    "\n",
    "                max_results = num_tweets,\n",
    "                since_id = since_id\n",
    "                )\n",
    "\n",
    "        # Get users list from the includes object\n",
    "        try:\n",
    "            users = {u[\"id\"]:u.data for u in results.includes['users']}\n",
    "        except KeyError:\n",
    "            users = {}\n",
    "        try:\n",
    "            allMedia = {m[\"media_key\"]: m.data for m in results.includes['media']}\n",
    "        except KeyError:\n",
    "            allMedia = {}    \n",
    "        try:\n",
    "            places = {p[\"id\"]: p.data for p in results.includes['places']}\n",
    "        except KeyError:\n",
    "            places = {}\n",
    "\n",
    "        \n",
    "        if results.data is not None:\n",
    "            for status in results.data:\n",
    "                #print(status)\n",
    "                count += 1\n",
    "                author_id = status['author_id']\n",
    "\n",
    "                media_key = None\n",
    "                if 'entities' in status and 'urls' in status['entities'] and len(status['entities']['urls']) > 1:\n",
    "                    if 'media_key' in status['entities']['urls'][1]:\n",
    "                        media_key = status['entities']['urls'][1]['media_key']\n",
    "\n",
    "                place_id = None\n",
    "                if 'geo' in status and 'place_id' in status['geo']:\n",
    "                    place_id = status['geo']['place_id']\n",
    "\n",
    "                User = users.get(author_id)\n",
    "                media = allMedia.get(media_key)\n",
    "                place = places.get(place_id)\n",
    "\n",
    "                status = json.dumps(status.data)\n",
    "                status = json.loads(status)\n",
    "                status['User'] = User\n",
    "                status['Media'] = media\n",
    "                status['Place'] = place\n",
    "                status['query'] = user\n",
    "                tweet_obj, user_obj, new_tweet = build_entry(topic, status, query_search, container, query_language=None, target_languages=target_languages, username=username)\n",
    "                all_tweets.append(tweet_obj)\n",
    "                print(new_tweet)\n",
    "                if new_tweet:\n",
    "                    all_users.append(user_obj) # todo: optimize so not re-inserting the same user\n",
    "                if datetime.strptime(status['created_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\") < end_date:\n",
    "                    break\n",
    "        else:\n",
    "            print(\"No results found.\")            \n",
    "        print(\"Found \"+str(count) +\" tweets for user: \"+ user)\n",
    "        return all_tweets, all_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = [\n",
    "#     ('مصرف قطر المركزي -RT',\"QCB\"),\n",
    "#     ('QCB Qatar -RT',\"QCB\"),\n",
    "#      ('\"Qatar Central Bank\" -RT',\"QCB\"),\n",
    "#     ('QCBQATRA',\"QCB\"),\n",
    "#      ('محافظ مصرف قطر المركزي -RT',\"المحافظ\"),\n",
    "#      ('بندر بن محمد بن سعود ال ثاني -RT',\"المحافظ\"),\n",
    "#      ('Bandar bin Mohammed bin Saoud Al-Thani -RT',\"المحافظ\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queries = [\n",
    "#    ('(\"مطار حمد الدولي\") -RT',\"HIA\")\n",
    "#     ('(\"Hamad International Airport\") -RT',\"HIA\"),\n",
    "#     ('(\"HIAQatar\") -RT',\"HIA\"),\n",
    "#     ('(Qatar(\"Flights delay\")) -RT',\"Flights delay\"),\n",
    "#     ('(Qatar Airport (\"customer service\")) -RT',\"customer service\"),\n",
    "#     ('(Qatar Airport (security)) -RT',\"security\"),\n",
    "#     ('(Qatar Airport (cancellations)) -RT',\"cancellations\"),\n",
    "#     ('(Qatar HIA (\"customer service\")) -RT',\"customer service\"),\n",
    "#     ('(Qatar HIA (security)) -RT',\"security\"),\n",
    "#     ('(Qatar HIA (cancellations)) -RT',\"cancellations\"),\n",
    "#     ('(Qatar HIA (refunds)) -RT',\"refunds\"),\n",
    "#     ('(\"Qatar Airways\") -RT',\"Qatar Airways\"),\n",
    "#     (\"qatarairways\",\"Qatar Airways\")\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    ('مزاولة الانشطة بدون ترخيص السعودية -RT',\"الانشطة\"),\n",
    "    \n",
    "     ('مزاولة الانشطة بعد انتهاء صلاحية الترخيص السعودية -RT',\"الانشطة\"),\n",
    "    ('مزاولة الانشطة الترخيص السعودية -RT',\"الانشطة\"),\n",
    "    ('مزاولة انشطة ترخيص السعودية -RT',\"الانشطة\"),\n",
    "    ('التعصب الرياضي السعودية -RT',\"التعصب\"),\n",
    "    ('مخالفة ضوابط التواصل الاجتماعي السعودية -RT',\"التواصل الاجتماعي\"),\n",
    "    ('ضوابط الانشطة الاعلامية السعودية -RT',\"الانشطة الاعلامية\"),\n",
    "    ('محتوى اعلامي منسوخ السعودية -RT',\"حقوق الملكية\"),\n",
    "    ('مخالفة حقوق ملكية فكرية السعودية -RT',\"حقوق الملكية\"),\n",
    "    ('IPTV السعودية -RT',\"حقوق الملكية\"),\n",
    "    ('تصوير ترخيص السعودية -RT',\"تصوير بدون ترخيص\"),\n",
    "    ('تصنيف عمري افلام السعودية -RT',\"لائحة التصنيف العمري\"),\n",
    "    ('تصنيف عمري العاب السعودية -RT',\"لائحة التصنيف العمري\"),\n",
    "    ('تصنيف عمري برامج مرئية السعودية -RT',\"لائحة التصنيف العمري\"),\n",
    "    ('اوقات عرض معتمدة سينما السعودية -RT',\"اوقات العرض\"),\n",
    "    ('اوقات عرض معتمدة سينمائية السعودية -RT',\"اوقات العرض\"),\n",
    "    ('تدني مستوى الخدمة السعودية -RT',\"تدني مستوى الخدمة\"),\n",
    "    ('تدني مستوى الخدمة مرخصة السعودية -RT',\"تدني مستوى الخدمة\"),\n",
    "    ('مستوى خدمة متدني السعودية -RT',\"تدني مستوى الخدمة\"),\n",
    "    ('اساءة ثوابت الاسلام السعودية -RT',\"ثوابت الشريعة\"),\n",
    "    ('اساءة ثوابت الشريعة السعودية -RT',\"ثوابت الشريعة\"),\n",
    "    ('تجريح ثوابت الشريعة السعودية -RT',\"ثوابت الشريعة\"),\n",
    "    ('تجريح ثوابت الاسلام السعودية -RT',\"ثوابت الشريعة\"),\n",
    "    ('الاخلال النظام العام السعودية -RT',\"مقتضيات المصلحة العامة\"),\n",
    "    ('الاخلال الامن الوطني السعودية -RT',\"مقتضيات المصلحة العامة\"),\n",
    "    ('الاخلال المصلحة العامة السعودية -RT',\"مقتضيات المصلحة العامة\"),\n",
    "    ('الضرر النظام العام السعودية -RT',\"مقتضيات المصلحة العامة\"),\n",
    "    ('الضرر الامن الوطني السعودية -RT',\"مقتضيات المصلحة العامة\"),\n",
    "    ('الضرر المصلحة العامة السعودية -RT',\"مقتضيات المصلحة العامة\"),\n",
    "    ('علاقات الدول الخارجية السعودية -RT',\"علاقات خارجية\"),\n",
    "    ('علاقات المملكة الخارجية السعودية -RT',\"علاقات خارجية\"),\n",
    "    ('اثارة النعرات السعودية -RT',\"الفرقة والكراهية\"),\n",
    "    ('اثارة الفرقة السعودية -RT',\"الفرقة والكراهية\"),\n",
    "    ('اثارة الكراهية السعودية -RT',\"الفرقة والكراهية\"),\n",
    "    ('محتوى اعلامي اداب عامة السعودية -RT',\"الاداب العامة\"),\n",
    "    ('محتوى اداب عامة السعودية -RT',\"الاداب العامة\"),\n",
    "    ('محتوى فواحش  السعودية -RT',\"الاداب العامة\"),\n",
    "    ('محتوى رزائل  السعودية -RT',\"الاداب العامة\"),\n",
    "    ('محتوى اداب لغة مبتذلة السعودية -RT',\"الاداب العامة\"),\n",
    "    ('محتوى سحر السعودية -RT',\" ترهيب وسحر وشعوذة\"),\n",
    "    ('محتوى شعوذة السعودية -RT',\" ترهيب وسحر وشعوذة\"),\n",
    "    ('محتوى ترهيب السعودية -RT',\" ترهيب وسحر وشعوذة\"),\n",
    "    ('محتوى الترهيب السعودية -RT',\" ترهيب وسحر وشعوذة\"),\n",
    "    ('محتوى السحر و الشعوذة السعودية -RT',\" ترهيب وسحر وشعوذة\"),\n",
    "    ('المساس حقوق المرأة السعودية -RT',\" حقوق المرأة والطفل\"),\n",
    "    ('المساس حقوق الطفل السعودية -RT',\" حقوق المرأة والطفل\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on topic: الانشطة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: مزاولة الانشطة بدون ترخيص السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الانشطة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: مزاولة الانشطة بعد انتهاء صلاحية الترخيص السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الانشطة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: مزاولة الانشطة الترخيص السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الانشطة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: مزاولة انشطة ترخيص السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: التعصب\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: التعصب الرياضي السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: التواصل الاجتماعي\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: مخالفة ضوابط التواصل الاجتماعي السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الانشطة الاعلامية\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: ضوابط الانشطة الاعلامية السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: حقوق الملكية\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى اعلامي منسوخ السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: حقوق الملكية\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: مخالفة حقوق ملكية فكرية السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: حقوق الملكية\n",
      "Start search\n",
      "After search/results var\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "Found 28 tweets for query: IPTV السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: تصوير بدون ترخيص\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: تصوير ترخيص السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: لائحة التصنيف العمري\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: تصنيف عمري افلام السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: لائحة التصنيف العمري\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: تصنيف عمري العاب السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: لائحة التصنيف العمري\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: تصنيف عمري برامج مرئية السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: اوقات العرض\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: اوقات عرض معتمدة سينما السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: اوقات العرض\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: اوقات عرض معتمدة سينمائية السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: تدني مستوى الخدمة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: تدني مستوى الخدمة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: تدني مستوى الخدمة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: تدني مستوى الخدمة مرخصة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: تدني مستوى الخدمة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: مستوى خدمة متدني السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: ثوابت الشريعة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: اساءة ثوابت الاسلام السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: ثوابت الشريعة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: اساءة ثوابت الشريعة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: ثوابت الشريعة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: تجريح ثوابت الشريعة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: ثوابت الشريعة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: تجريح ثوابت الاسلام السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: مقتضيات المصلحة العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: الاخلال النظام العام السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: مقتضيات المصلحة العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: الاخلال الامن الوطني السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: مقتضيات المصلحة العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: الاخلال المصلحة العامة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: مقتضيات المصلحة العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: الضرر النظام العام السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: مقتضيات المصلحة العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: الضرر الامن الوطني السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: مقتضيات المصلحة العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: الضرر المصلحة العامة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: علاقات خارجية\n",
      "Start search\n",
      "After search/results var\n",
      "New Tweet\n",
      "Found 1 tweets for query: علاقات الدول الخارجية السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: علاقات خارجية\n",
      "Start search\n",
      "After search/results var\n",
      "New Tweet\n",
      "New Tweet\n",
      "Found 2 tweets for query: علاقات المملكة الخارجية السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الفرقة والكراهية\n",
      "Start search\n",
      "After search/results var\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "New Tweet\n",
      "Found 11 tweets for query: اثارة النعرات السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الفرقة والكراهية\n",
      "Start search\n",
      "After search/results var\n",
      "New Tweet\n",
      "Found 1 tweets for query: اثارة الفرقة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الفرقة والكراهية\n",
      "Start search\n",
      "After search/results var\n",
      "New Tweet\n",
      "Found 1 tweets for query: اثارة الكراهية السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الاداب العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى اعلامي اداب عامة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الاداب العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى اداب عامة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الاداب العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى فواحش  السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الاداب العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى رزائل  السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic: الاداب العامة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى اداب لغة مبتذلة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic:  ترهيب وسحر وشعوذة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى سحر السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic:  ترهيب وسحر وشعوذة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى شعوذة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic:  ترهيب وسحر وشعوذة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى ترهيب السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic:  ترهيب وسحر وشعوذة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى الترهيب السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic:  ترهيب وسحر وشعوذة\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: محتوى السحر و الشعوذة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic:  حقوق المرأة والطفل\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: المساس حقوق المرأة السعودية -RT\n",
      "Inserted data to cosmos\n",
      "Working on topic:  حقوق المرأة والطفل\n",
      "Start search\n",
      "After search/results var\n",
      "No results found.\n",
      "Found 0 tweets for query: المساس حقوق الطفل السعودية -RT\n",
      "Inserted data to cosmos\n"
     ]
    }
   ],
   "source": [
    "for query, topic in queries:\n",
    "    # Set the since_datetime to be 7 days in the past from the current date and time\n",
    "    since_datetime = datetime.utcnow() - timedelta(days=7)\n",
    "\n",
    "    # Get the top 1 tweet from Cosmos DB that meets the criteria\n",
    "    cosmos_query = f\"SELECT TOP 1 c.originalid, c.created_at FROM c WHERE c.document_type = 'tweet' and c.query= '{query}' and c.created_at > '{since_datetime.isoformat()}' ORDER BY c.created_at DESC\"\n",
    "    items = list(tweet_container_client.query_items(cosmos_query, enable_cross_partition_query=True)) \n",
    "\n",
    "    # Retrieve the tweet ID if available\n",
    "    if items:\n",
    "        since_id = str(items[0]['originalid'])\n",
    "    else:\n",
    "        # Handle the case when no valid tweet ID is available\n",
    "        since_id = None\n",
    "\n",
    "    if users == \"\": # query search\n",
    "        all_tweets, all_users = process_tweets(topic, query, query_language, maxdays=max_days, maxtweets_persearch=num_tweets, user=\"\", query_search=True, container=tweet_container_client, target_languages=target_languages)\n",
    "        update_cosmos(all_tweets, tweet_container_client) # insert tweets\n",
    "        #update_cosmos(all_users, tweet_container_client) # insert users\n",
    "    else: # user search\n",
    "        for usr in userslist:\n",
    "            all_tweets, all_users = process_tweets(topic, user=usr, maxdays=max_days, maxtweets_persearch=num_tweets, query_search=False, container=tweet_container_client, target_languages=target_languages, username=usr)\n",
    "            update_cosmos(all_tweets, tweet_container_client) # insert tweets\n",
    "            #update_cosmos(all_users, tweet_container_client) # insert users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
